{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c240c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python3 -m venv ../../myenv\n",
    "%%source ../../myenv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420afc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8587441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89a9b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../data\"\n",
    "test_file_path = os.path.join(data_folder, \"merged_instructions.csv\")\n",
    "df = pd.read_csv(test_file_path)\n",
    "df['instruction'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b28faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "result_data = []\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region of your choice.\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "# Set the model ID, e.g., Llama 3 70b Instruct.\n",
    "model_id = \"us.meta.llama3-2-11b-instruct-v1:0\"\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    message = row['instruction']\n",
    "\n",
    "    prompt = f\"You are a helpful assistant that generates code snippets based on user instructions.\" \\\n",
    "                \"The method signature should be according to the given in the example part.\\n\" \\\n",
    "                \"Use the following guidlines in generation: \\n\" \\\n",
    "                \" - Provide the code only\\n\" \\\n",
    "                \" - Do not output any other texts or test cases results.\\n\" \\\n",
    "                \" - Include the necessary import statements on the top even if they are usually not needed like - re, typing, itertools, Split etc. \\n\" \\\n",
    "                \" - Do not include any comments in the code\\n\" \\\n",
    "                \" - Do not forget to consider the edge cases\\n\" \\\n",
    "                \" - Avoid recursion if possible\\n\" \\\n",
    "                \" - Lastly, verify the code has no compilations error and contains the necessary imports.\\n\"\n",
    "    \n",
    "\n",
    "    # Embed the prompt in Llama 3's instruction format.\n",
    "    formatted_prompt = f\"\"\"\n",
    "    <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "    {prompt + message}\n",
    "    <|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the request payload using the model's native structure.\n",
    "    native_request = {\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"max_gen_len\": 1024,\n",
    "        \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "    # Convert the native request to JSON.\n",
    "    request = json.dumps(native_request)\n",
    "\n",
    "    try:\n",
    "        # Invoke the model with the request.\n",
    "        response = client.invoke_model(modelId=model_id, body=request)\n",
    "\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Decode the response body.\n",
    "    model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    output = model_response[\"generation\"]\n",
    "\n",
    "    result_data.append({\n",
    "        \"id\": row['id'],\n",
    "        \"response\": output\n",
    "    })\n",
    "\n",
    "    print(\"Completed:\", row['id'])\n",
    "\n",
    "result_df = pd.DataFrame(result_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db08af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a json from the result_df\n",
    "result_json = result_df.to_json(orient=\"records\", indent=4)\n",
    "# save json in the data_folder os.path.join(data_folder, \"submission.json\")\n",
    "with open(\"llama_3.2_11B_submission.json\", \"w\") as json_file:\n",
    "    json_file.write(result_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
