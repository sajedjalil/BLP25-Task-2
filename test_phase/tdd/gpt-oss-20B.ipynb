{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8587441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f89a9b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    স্ট্রিং থেকে প্রদত্ত অক্ষরের প্রথম এবং শেষ উপস...\n",
       "1    একটি প্রদত্ত ম্যাট্রিক্সকে তার সারিগুলির যোগফল...\n",
       "2    একটি ফাংশন লিখুন যা একটি অভিধানে সবচেয়ে সাধার...\n",
       "3    একটি ত্রিভুজাকার প্রিজমের আয়তন খুঁজে বের করার...\n",
       "4    একটি স্ট্রিংকে ছোট অক্ষরে বিভক্ত করার জন্য একট...\n",
       "Name: instruction, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = \"../data\"\n",
    "test_file_path = os.path.join(data_folder, \"merged_instructions.csv\")\n",
    "df = pd.read_csv(test_file_path)\n",
    "df['instruction'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e581daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case generator Agent\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "def generate_test_cases(instruction) -> str:\n",
    "    model_id = 'openai.gpt-oss-120b-1:0'\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a test case generator. Your task is to generate test cases for the given method signature.\\n\n",
    "    - The test cases should be in the form of input-output pairs with assert statements.\\n\n",
    "    - The input should be a valid input for the instruction, and the output should be the expected output for that input.\\n\n",
    "    - Each test case should be on a separate line.\\n\n",
    "    - The test cases should be in python.\\n\n",
    "    - Consider edge cases and typical use cases.\\n\n",
    "    - Generate upto 5 test cases.\\n\n",
    "    - Do not include any explanations or additional text.\\n\n",
    "    \"\"\"\n",
    "    # Set up messages and system message\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": instruction\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    system = [\n",
    "        {\n",
    "            \"text\": prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            system=system,\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": 1024, \n",
    "                \"temperature\": 0.5\n",
    "            },\n",
    "        )\n",
    "\n",
    "        for content_block in response[\"output\"][\"message\"][\"content\"]:\n",
    "            if content_block.get(\"text\"):\n",
    "                return content_block.get(\"text\")\n",
    "\n",
    "\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b28faa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: 1\n",
      "Completed: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result_data = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    message = row['instruction']\n",
    "\n",
    "    test_case_assert_statements = generate_test_cases(message)\n",
    "\n",
    "    if test_case_assert_statements:\n",
    "        message += \"\\n Your goal is to pass the following test cases:\\n\" + test_case_assert_statements # add test cases if generated\n",
    "\n",
    "    model_id = 'openai.gpt-oss-20b-1:0'\n",
    "\n",
    "    prompt = f\"You are a helpful assistant that generates code snippets based on user instructions.\" \\\n",
    "                \"The method signature should be according to the given in the example part.\\n\" \\\n",
    "                \"Use the following guidlines in generation: \\n\" \\\n",
    "                \" - Provide the code in the response only\\n\" \\\n",
    "                \" - Do not output any other texts or test cases results.\\n\" \\\n",
    "                \" - Include the necessary import statements on the top even if they are usually not needed like - re, typing, itertools, Split etc. \\n\" \\\n",
    "                \" - Do not include any comments in the code\\n\" \\\n",
    "                \" - Do not forget to consider the edge cases\\n\" \\\n",
    "                \" - Avoid recursion if possible\\n\" \\\n",
    "                \" - Lastly, verify the code has no compilations error and contains the necessary imports.\\n\"\n",
    "    \n",
    "    # Set up messages and system message\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": message\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    system = [\n",
    "        {\n",
    "            \"text\": prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Send the message to the model, using a basic inference configuration.\n",
    "        response = client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=messages,\n",
    "            system=system,\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": 1024, \n",
    "                \"temperature\": 0.5\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # Extract and print the response text.\n",
    "        flag = False\n",
    "        for content_block in response[\"output\"][\"message\"][\"content\"]:\n",
    "            if content_block.get(\"text\"):\n",
    "                code_response = content_block.get(\"text\")\n",
    "                result_data.append({\n",
    "                    \"id\": row['id'],\n",
    "                    \"response\": code_response\n",
    "                })\n",
    "                flag = True\n",
    "\n",
    "        if not flag:\n",
    "            result_data.append({\n",
    "                    \"id\": row['id'],\n",
    "                    \"response\": \"\"\n",
    "                })\n",
    "\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "\n",
    "    print(\"Completed:\", row['id'])\n",
    "    \n",
    "result_df = pd.DataFrame(result_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db08af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a json from the result_df\n",
    "result_json = result_df.to_json(orient=\"records\", indent=4)\n",
    "# save json in the data_folder os.path.join(data_folder, \"submission.json\")\n",
    "with open(\"gpt_oss_20B_submission.json\", \"w\") as json_file:\n",
    "    json_file.write(result_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
